package net.jpmchase.payroll.processor.utils;

import net.jpmc.ccb.business_banking.payments.payroll_management.PayrollManagementProcessingEventReceived;
import net.jpmchase.payroll.processor.mapper.EventMapper;
import net.jpmchase.payroll.processor.model.Event;
import org.apache.avro.io.BinaryEncoder;
import org.apache.avro.io.EncoderFactory;
import org.apache.avro.specific.SpecificDatumWriter;
import org.apache.kafka.clients.consumer.ConsumerRecord;

import java.io.ByteArrayOutputStream;
import java.nio.ByteBuffer;

public class SampleKafkaProducer {

    public static void main(String[] args) throws Exception {

        // 1) Build a sample Avro event (use whatever values you like)
        PayrollManagementProcessingEventReceived event =
                PayrollManagementProcessingEventReceived.newBuilder()
                        .setEventIdentifier("7714d40b-7f8b-48aa-880b-9827c9240cb2")
                        .setEventProcessTimestamp(System.currentTimeMillis())
                        .setEventTypeName("payroll.processed")
                        .setEntityTypeName("PAYROLL")
                        .setEntityIdentifier("41d20cdc-3694-4426-8897-8eaebb01e32c")
                        .setResourceTypeName("COMPANY")
                        .setResourceIdentifier("ab80909b-08ee-4c3a-b578-c084f1ce04ff")
                        .build();

        // 2) Serialize to **raw Avro bytes** (no Confluent header yet)
        byte[] rawAvroBytes;
        try (ByteArrayOutputStream out = new ByteArrayOutputStream()) {
            SpecificDatumWriter<PayrollManagementProcessingEventReceived> writer =
                    new SpecificDatumWriter<>(PayrollManagementProcessingEventReceived.class);

            BinaryEncoder encoder = EncoderFactory.get().binaryEncoder(out, null);
            writer.write(event, encoder);
            encoder.flush();

            rawAvroBytes = out.toByteArray();
        }

        // 3) Wrap in **Confluent wire format**: [magic (1 byte)] + [schemaId (4 bytes)] + [raw Avro]
        // Use any dummy schema id here â€“ your decoder will ignore it and just skip the first 5 bytes.
        int schemaId = 1;
        ByteBuffer buffer = ByteBuffer.allocate(1 + 4 + rawAvroBytes.length);
        buffer.put((byte) 0);      // magic byte
        buffer.putInt(schemaId);   // schema id
        buffer.put(rawAvroBytes);  // real Avro payload

        byte[] confluentBytes = buffer.array();

        // 4) Build a fake ConsumerRecord with the Confluent-formatted payload
        ConsumerRecord<String, byte[]> record =
                new ConsumerRecord<>("test-topic", 0, 0L, "key", confluentBytes);

        // (Optional) add headers if you want to simulate real message headers
        // record.headers().add("x-correlation-id", "local-test".getBytes(StandardCharsets.UTF_8));
        // KsaasHeaderUtil.addHeaders(record, ...);

        // 5) Call your EventMapper / listener to verify the decoding logic
        EventMapper mapper = new EventMapper();
        Event mapped = mapper.fromRecord(record);   // this should call your decode() that skips 5 bytes

        System.out.println("Decoded event = " + mapped);
    }
}
@Configuration
public class KafkaAuthRetryCustomizer {

    @Value("${kafka.auth-exception-retry-interval-seconds:60}")
    private long authExceptionRetryIntervalSeconds;

    // Inject the existing factory created by Boot / your config
    public KafkaAuthRetryCustomizer(
            ConcurrentKafkaListenerContainerFactory<?, ?> kafkaListenerContainerFactory) {

        kafkaListenerContainerFactory.setContainerCustomizer(container ->
                container.getContainerProperties()
                         .setAuthExceptionRetryInterval(
                                 Duration.ofSeconds(authExceptionRetryIntervalSeconds)));
    }
}

package com.yourcompany.yourapp.config;

import java.time.Duration;

import org.springframework.beans.factory.annotation.Value;
import org.springframework.boot.autoconfigure.kafka.ConcurrentKafkaListenerContainerFactoryConfigurer;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
import org.springframework.kafka.config.ConcurrentKafkaListenerContainerFactory;
import org.springframework.kafka.core.ConsumerFactory;

@Configuration
public class KafkaAuthRetryConfig {

    /**
     * Custom property from application-*.yml.
     * Default 60s to avoid NPE if the property is not set.
     */
    @Value("${kafka.auth-exception-retry-interval-seconds:60}")
    private long authExceptionRetryIntervalSeconds;

    /**
     * Override the default kafkaListenerContainerFactory bean while still
     * letting Spring Boot apply all spring.kafka.* properties for you.
     */
    @Bean
    public ConcurrentKafkaListenerContainerFactory<Object, Object> kafkaListenerContainerFactory(
            ConcurrentKafkaListenerContainerFactoryConfigurer configurer,
            ConsumerFactory<Object, Object> consumerFactory) {

        ConcurrentKafkaListenerContainerFactory<Object, Object> factory =
                new ConcurrentKafkaListenerContainerFactory<>();

        // This line applies all your existing spring.kafka.* config
        configurer.configure(factory, consumerFactory);

        // ðŸ”‘ This is the important part: set authExceptionRetryInterval via a container customizer
        factory.setContainerCustomizer(container ->
                container.getContainerProperties()
                         .setAuthExceptionRetryInterval(
                                 Duration.ofSeconds(authExceptionRetryIntervalSeconds)));

        return factory;
    }
}
